{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from random import sample\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "current_dir = os.path.dirname(os.getcwd())\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "from otodom.models.architecture import ConvNet2d\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and normalize images\n",
    "# Plans\n",
    "images_dir = os.path.join('..', 'images', 'plans', '*.jpg')\n",
    "image_paths_plans = glob.glob(images_dir)\n",
    "\n",
    "# All other\n",
    "image_paths_all = glob.glob(os.path.join('..', 'images', 'other', '*.jpg'))\n",
    "\n",
    "# Bathrooms\n",
    "image_paths_bathrooms = glob.glob(os.path.join('..', 'images', 'bathroom', '*.jpg'))\n",
    "image_pahts_other = image_paths_all + image_paths_bathrooms  \n",
    "\n",
    "# False positives\n",
    "images_fp_dir = os.path.join('..', 'images', 'plans', 'plan_fp', '*.jpg')\n",
    "image_paths_other = glob.glob(images_fp_dir)\n",
    "\n",
    "# False positives test set\n",
    "images_fp_test_dir = os.path.join('..', 'images', 'plans', 'plan_fp_test', '*.jpg')\n",
    "image_fp_test_paths = glob.glob(images_fp_test_dir)\n",
    "\n",
    "# filename = 'plan_classifier.sav'\n",
    "# clf = joblib.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image_size(input_image: np.array, desired: int = 600, shape: int = 3):\n",
    "    def calculate_padding_top_bottom(current: int, desired=desired) -> (int,int):\n",
    "        if desired == current:\n",
    "            top, bottom = 0, 0\n",
    "        else:\n",
    "            pad = desired-current\n",
    "            top, bottom = int(np.ceil(pad/2)), int(np.floor(pad/2))\n",
    "        return top, bottom\n",
    "\n",
    "    i = input_image.copy()\n",
    "    h, w = i.shape[:2]\n",
    "    \n",
    "    # Top bottom\n",
    "    pad_top, pad_bottom = calculate_padding_top_bottom(current=h)\n",
    "    if shape == 3:\n",
    "        if pad_bottom >= 0:\n",
    "            i = cv2.copyMakeBorder(i, pad_top, pad_bottom, 0, 0, cv2.BORDER_CONSTANT, None, [0,0,0])\n",
    "        else:\n",
    "            pad_top, pad_bottom = np.abs(pad_top), np.abs(pad_bottom)\n",
    "            i = i[pad_top:h-pad_bottom, :, :]\n",
    "\n",
    "        # Left Right\n",
    "        pad_left, pad_right = calculate_padding_top_bottom(current=w)\n",
    "        if pad_right >= 0:\n",
    "            i = cv2.copyMakeBorder(i, 0, 0, pad_left, pad_right, cv2.BORDER_CONSTANT, None, [0,0,0])\n",
    "        else:\n",
    "            pad_left, pad_right = np.abs(pad_left), np.abs(pad_right)\n",
    "            i = i[:, pad_left:w-pad_right, :]\n",
    "        return i\n",
    "    elif shape == 2:\n",
    "        if pad_bottom >= 0:\n",
    "            i = cv2.copyMakeBorder(i, pad_top, pad_bottom, 0, 0, cv2.BORDER_CONSTANT, None, [0,0])\n",
    "        else:\n",
    "            pad_top, pad_bottom = np.abs(pad_top), np.abs(pad_bottom)\n",
    "            i = i[pad_top:h-pad_bottom, :]\n",
    "\n",
    "        # Left Right\n",
    "        pad_left, pad_right = calculate_padding_top_bottom(current=w)\n",
    "        if pad_right >= 0:\n",
    "            i = cv2.copyMakeBorder(i, 0, 0, pad_left, pad_right, cv2.BORDER_CONSTANT, None, [0,0])\n",
    "        else:\n",
    "            pad_left, pad_right = np.abs(pad_left), np.abs(pad_right)\n",
    "            i = i[:, pad_left:w-pad_right]\n",
    "        return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model: nn.Module, data_loader: torch.utils.data.DataLoader):\n",
    "    \n",
    "    # Test the model\n",
    "    model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "    results_true = np.array([])\n",
    "    results_pred = np.array([])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            results_true = np.concatenate((results_true, labels), axis=0)\n",
    "            results_pred = np.concatenate((results_pred, predicted), axis=0)\n",
    "            for image, label, prediction in zip(images, labels, predicted):\n",
    "#                 plt.imshow(image[0])\n",
    "#                 plt.show()\n",
    "                print(f\"True: {label}, Predicted: {prediction}\")\n",
    "\n",
    "        print('Test Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
    "    return results_true, results_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ckp(state,\n",
    "             is_best,\n",
    "             model_name,\n",
    "             checkpoint_dir=\"../models/checkpoint\"):\n",
    "    f_path = os.path.join(checkpoint_dir,f'{model_name}_checkpoint.pt')\n",
    "    torch.save(state, f_path)\n",
    "    if is_best:\n",
    "        best_fpath = os.path.join(checkpoint_dir,f'{model_name}_best_model.pt')\n",
    "        shutil.copyfile(f_path, best_fpath)\n",
    "        \n",
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    return model, optimizer, checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Sklearn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def draw_sample(paths: list, X: list, y: list, class_label: int):\n",
    "    for img in paths:\n",
    "        i = cv2.imread(img)\n",
    "        i_hsv = cv2.cvtColor(i, cv2.COLOR_BGR2HSV)\n",
    "        img_normalized = normalize_image_size(input_image=i_hsv)    \n",
    "        X.append(img_normalized.ravel())\n",
    "        y.append(class_label)\n",
    "    return None\n",
    "    \n",
    "split = 50\n",
    "image_paths_plans_train = image_paths_plans[:-split]\n",
    "image_paths_plans_test = image_paths_plans[-split:]\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Create data set\n",
    "draw_sample(paths=image_paths_plans_train, X=X, y=y, class_label=1)\n",
    "draw_sample(paths=sample(image_paths_other,\n",
    "                         len(image_paths_plans_train)),\n",
    "                            X=X,\n",
    "                            y=y,\n",
    "                         class_label=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. RFC | thr = 0.6 | n_est = 1000 | max_d = 6| acc total = 0.93 | class 1 prec = 0.95 | class 1 recall = 0.88\n",
    "2. RFC | thr = 0.6 | n_est = 1200 | max_d = 5| acc total = 0.93 | class 1 prec = 0.95 | class 1 recall = 0.88\n",
    "2. RFC | thr = 0.55 | n_est = 1500 | max_d = 5| acc total = 0.94 | class 1 prec = 0.95 | class 1 recall = 0.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "clf = RandomForestClassifier(n_estimators=1500, max_depth=5).fit(X_train, y_train)\n",
    "# clf = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
    "# clf.fit(np.array(X_train), np.array(y_train))\n",
    "yhat_test_proba = clf.predict_proba(X_test)\n",
    "yhat_test = (yhat_test_proba[:,1] >= 0.55).astype('int')\n",
    "\n",
    "class_report = classification_report(y_true=y_test, y_pred=yhat_test)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_paths_white = ['../images/other/548353190_3.jpg',\n",
    "                     '../images/other/14837540_0.jpg',\n",
    "                     '../images/other/1006482452650910471374309_1.jpg',\n",
    "                     '../images/other/574814704_0.jpg',\n",
    "                     '../images/other/14886494_2.jpg',\n",
    "                     '../images/other/14407902_6.jpg',\n",
    "                     '../images/other/1006509185630911379840309_2.jpg',\n",
    "                     '../images/other/14251019_10.jpg',\n",
    "                     '../images/other/573866182_3.jpg',\n",
    "                     '../images/other/550327539_6.jpg',\n",
    "                     '../images/other/1006482452650910471374309_7.jpg',\n",
    "                     # HSV\n",
    "                     '../images/other/1006515348390911381140509_2.jpg',\n",
    "                    ]\n",
    "\n",
    "paths_other = sample(image_paths_other, split)\n",
    "image_paths_test = image_paths_plans_test + image_paths_white + paths_other\n",
    "\n",
    "for img in image_paths_test:\n",
    "    i = cv2.imread(img)\n",
    "    i = cv2.cvtColor(i, cv2.COLOR_BGR2HSV)\n",
    "    img_normalized = normalize_image_size(input_image=i)    \n",
    "#     y_hat = clf.predict([img_normalized.ravel()])\n",
    "    y_hat_proba = clf.predict_proba([img_normalized.ravel()])\n",
    "    y_hat_test = (y_hat_proba[:,1] >= 0.55).astype('int')\n",
    "    if y_hat_test == 1:\n",
    "#     if True:\n",
    "        plt.imshow(i)\n",
    "        plt.show()\n",
    "        print(y_hat_proba)\n",
    "        print(y_hat_test)\n",
    "        print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # save the model to disk\n",
    "# filename = 'plan_classifier.sav'\n",
    "# joblib.dump(clf, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Mistakes to be improved\n",
    "- white apartments\n",
    "- plans pictures with bad quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# ConvNet Implementation\n",
    "\n",
    "Good source: [gh](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/convolutional_neural_network/main.py#L35-L56)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# Hyper parameters\n",
    "num_epochs = 10\n",
    "num_classes = 2\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "# Specify desired image format\n",
    "desired = 600\n",
    "# Specify desired size of test set\n",
    "split = 20\n",
    "read_model = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def draw_sample_conv(paths: list, X: list, y: list, class_label: int, dim: int = 2):\n",
    "    for img in paths:\n",
    "        if dim == 2:\n",
    "            i = cv2.imread(img, cv2.IMREAD_GRAYSCALE)\n",
    "            i = i.astype('float32')\n",
    "            i /= 255.0\n",
    "            img_normalized = normalize_image_size(input_image=i, shape=dim, desired=desired)    \n",
    "        elif dim == 3:\n",
    "            i = cv2.imread(img)\n",
    "            i = cv2.cvtColor(i, cv2.COLOR_BGR2HSV)\n",
    "            i = i.astype('float32')\n",
    "            i /= np.array([180.0, 255.0, 255.0])\n",
    "            img_normalized = normalize_image_size(input_image=i, shape=dim, desired=desired)    \n",
    "        X.append(img_normalized)\n",
    "        y.append(class_label)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "# Specify sample size\n",
    "if len(image_paths_plans) >= len(image_paths_other):\n",
    "    sample_size = len(image_paths_other)\n",
    "else:\n",
    "    sample_size = len(image_paths_plans)\n",
    "\n",
    "# Create TRAIN and VAL data sets\n",
    "draw_sample_conv(paths=sample(image_paths_plans, sample_size),\n",
    "                 X=X,\n",
    "                 y=y,\n",
    "                 class_label=1)\n",
    "draw_sample_conv(paths=sample(image_paths_other, sample_size),\n",
    "                 X=X,\n",
    "                 y=y,\n",
    "                 class_label=0)\n",
    "\n",
    "# Create TEST data set. So far all images are 0 class\n",
    "X_test = []\n",
    "Y_test = []\n",
    "draw_sample_conv(image_fp_test_paths,\n",
    "                 X=X_test,\n",
    "                 y=Y_test,\n",
    "                 class_label=0)\n",
    "\n",
    "# Convert to array, because this is what PyTorch expects\n",
    "X = np.array(X)\n",
    "# Split on test and val sets\n",
    "train_x, val_x, train_y, val_y = train_test_split(X, y, test_size = 0.15, shuffle=True)\n",
    "# Reshape for a format expected by pytorch, 1 is for grayscale images\n",
    "train_x = train_x.reshape(train_x.shape[0], 1, desired, desired)\n",
    "train_x = torch.from_numpy(train_x)\n",
    "val_x = val_x.reshape(val_x.shape[0], 1, desired, desired)\n",
    "val_x = torch.from_numpy(val_x)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "test_x = X_test.reshape(X_test.shape[0], 1, desired, desired)\n",
    "test_x = torch.from_numpy(test_x)\n",
    "test_y = Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Quality check to see if everything was read correctly\n",
    "# for image, label in zip(val_x[:5], val_y[:5]):\n",
    "#     plt.imshow(image[0])\n",
    "#     plt.show()\n",
    "#     print(f\"True: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Convert to pyTorch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "for (x,y) in zip(train_x, train_y):\n",
    "    train_dataset.append((x,y))\n",
    "\n",
    "val_dataset = []\n",
    "for (x,y) in zip(val_x, val_y):\n",
    "    val_dataset.append((x,y))\n",
    "\n",
    "test_dataset = []\n",
    "for (x,y) in zip(test_x, test_y):\n",
    "    test_dataset.append((x,y))\n",
    "\n",
    "    \n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                         batch_size=batch_size, \n",
    "                                         shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Prepare model architecture\n",
    "\n",
    "Nex layer size: (img_width - kernel_width + 2*padding)/stride + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Modify class in models.architecture file\n",
    "\n",
    "# class ConvNet(nn.Module):\n",
    "#     def __init__(self, num_classes=2):\n",
    "#         super(ConvNet, self).__init__()\n",
    "#         self.layer1 = nn.Sequential(\n",
    "#             nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "#             nn.BatchNorm2d(16),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "#         self.layer2 = nn.Sequential(\n",
    "#             nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "#             nn.BatchNorm2d(32),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "#         self.fc = nn.Linear(150*150*32, num_classes)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         out = self.layer1(x)\n",
    "#         out = self.layer2(out)\n",
    "#         out = out.reshape(out.size(0), -1)\n",
    "#         out = self.fc(out)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Read model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    return model, optimizer, checkpoint['epoch']\n",
    "\n",
    "if read_model:\n",
    "    ckp_path = \"../models/checkpoint/checkpoint.pt\"\n",
    "    model = ConvNet(num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model, optimizer, start_epoch = load_ckp(ckp_path, model, optimizer)\n",
    "else:\n",
    "    model = ConvNet(num_classes).to(device)\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "               .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_model(model: nn.Module, data_loader: torch.utils.data.DataLoader):\n",
    "    \n",
    "    # Test the model\n",
    "    model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "    results_true = np.array([])\n",
    "    results_pred = np.array([])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            results_true = np.concatenate((results_true, labels), axis=0)\n",
    "            results_pred = np.concatenate((results_pred, predicted), axis=0)\n",
    "            for image, label, prediction in zip(images, labels, predicted):\n",
    "#                 plt.imshow(image[0])\n",
    "#                 plt.show()\n",
    "                print(f\"True: {label}, Predicted: {prediction}\")\n",
    "\n",
    "        print('Test Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
    "    return results_true, results_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results_true, results_pred = test_model(model=model, data_loader=val_loader)\n",
    "cr = classification_report(y_true=results_true, y_pred=results_pred)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results_true, results_pred = test_model(model=model, data_loader=test_loader)\n",
    "cr = classification_report(y_true=results_true, y_pred=results_pred)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def save_ckp(state,\n",
    "             is_best,\n",
    "             model_name,\n",
    "             checkpoint_dir=\"../models/checkpoint\"):\n",
    "    f_path = os.path.join(checkpoint_dir,f'{model_name}_checkpoint.pt')\n",
    "    torch.save(state, f_path)\n",
    "    if is_best:\n",
    "        best_fpath = os.path.join(checkpoint_dir,f'{model_name}_best_model.pt')\n",
    "        shutil.copyfile(f_path, best_fpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'epoch': epoch + 1,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict()\n",
    "}\n",
    "save_ckp(state=checkpoint, is_best=True, model_name=\"2d_conv_net\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNet 3 dimensional images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_sample_conv(paths: list, X: list, y: list, class_label: int, dim: int = 2):\n",
    "    for img in paths:\n",
    "        if dim == 2:\n",
    "            i = cv2.imread(img, cv2.IMREAD_GRAYSCALE)\n",
    "            i = i.astype('float32')\n",
    "            i /= 255.0\n",
    "            img_normalized = normalize_image_size(input_image=i, shape=dim, desired=desired)    \n",
    "        elif dim == 3:\n",
    "            i = cv2.imread(img)\n",
    "            i = cv2.cvtColor(i, cv2.COLOR_BGR2HSV)\n",
    "            i = i.astype('float32')\n",
    "            i = cv2.normalize(i, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "            img_normalized = normalize_image_size(input_image=i, shape=dim, desired=desired)    \n",
    "        X.append(img_normalized)\n",
    "        y.append(class_label)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# Hyper parameters\n",
    "num_epochs = 20\n",
    "num_classes = 2\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "# Specify desired image format\n",
    "desired = 400\n",
    "# Specify desired size of test set\n",
    "split = 20\n",
    "read_model = True\n",
    "dim=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "# Specify sample size\n",
    "if len(image_paths_plans) >= len(image_paths_other):\n",
    "    sample_size = len(image_paths_other)\n",
    "else:\n",
    "    sample_size = len(image_paths_plans)\n",
    "\n",
    "# Create TRAIN and VAL data sets\n",
    "draw_sample_conv(paths=sample(image_paths_plans, sample_size),\n",
    "                 X=X,\n",
    "                 y=y,\n",
    "                 dim=dim,\n",
    "                 class_label=1)\n",
    "draw_sample_conv(paths=sample(image_paths_other, sample_size),\n",
    "                 X=X,\n",
    "                 y=y,\n",
    "                 dim=dim,\n",
    "                 class_label=0)\n",
    "\n",
    "# Create TEST data set. So far all images are 0 class\n",
    "X_test = []\n",
    "Y_test = []\n",
    "draw_sample_conv(image_fp_test_paths,\n",
    "                 X=X_test,\n",
    "                 y=Y_test,\n",
    "                 dim=dim,\n",
    "                 class_label=0)\n",
    "\n",
    "# Convert to array, because this is what PyTorch expects\n",
    "X = np.array(X)\n",
    "# Split on test and val sets\n",
    "train_x, val_x, train_y, val_y = train_test_split(X, y, test_size = 0.15, shuffle=True)\n",
    "# Reshape for a format expected by pytorch, 3 is for color images\n",
    "train_x = train_x.reshape(train_x.shape[0], dim, desired, desired)\n",
    "train_x = torch.from_numpy(train_x)\n",
    "val_x = val_x.reshape(val_x.shape[0], dim, desired, desired)\n",
    "val_x = torch.from_numpy(val_x)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "test_x = X_test.reshape(X_test.shape[0], dim, desired, desired)\n",
    "test_x = torch.from_numpy(test_x)\n",
    "test_y = Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to PyTorch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "for (x,y) in zip(train_x, train_y):\n",
    "    train_dataset.append((x,y))\n",
    "\n",
    "val_dataset = []\n",
    "for (x,y) in zip(val_x, val_y):\n",
    "    val_dataset.append((x,y))\n",
    "\n",
    "test_dataset = []\n",
    "for (x,y) in zip(test_x, test_y):\n",
    "    test_dataset.append((x,y))\n",
    "\n",
    "    \n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                         batch_size=batch_size, \n",
    "                                         shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet3d(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ConvNet3d, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 8, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(50*50*8, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_model:\n",
    "    ckp_path = \"../models/checkpoint/3d_conv_net_3_layers_checkpoint.pt\"\n",
    "    model = ConvNet3d(num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model, optimizer, start_epoch = load_ckp(ckp_path, model, optimizer)\n",
    "else:\n",
    "    model = ConvNet3d(num_classes).to(device)\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Quality check to see if everything was read correctly\n",
    "# for image, label in zip(val_x[:5], val_y[:5]):\n",
    "#     plt.imshow(image[0])\n",
    "#     plt.show()\n",
    "#     print(f\"True: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "if not 'logs' in os.listdir():\n",
    "    os.mkdir('logs')\n",
    "    \n",
    "writer = SummaryWriter(os.path.join(os.getcwd(), 'logs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "writer.add_image('plan_images', img_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def images_to_probs(model, images):\n",
    "    '''\n",
    "    Generates predictions and corresponding probabilities from a trained\n",
    "    network and a list of images\n",
    "    \n",
    "    :model: trained PyTorch model \n",
    "    :images: batch of images\n",
    "    '''\n",
    "    output = model(images)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "    preds = np.squeeze(preds_tensor.numpy())\n",
    "    return preds, preds_tensor\n",
    "\n",
    "\n",
    "def plot_classes_preds(net, images, labels):\n",
    "    '''\n",
    "    Generates matplotlib Figure using a trained network, along with images\n",
    "    and labels from a batch, that shows the network's top prediction along\n",
    "    with its probability, alongside the actual label, coloring this\n",
    "    information based on whether the prediction was correct or not.\n",
    "    Uses the \"images_to_probs\" function.\n",
    "    '''\n",
    "    preds, probs = images_to_probs(net, images)\n",
    "    # plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(12, 48))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
    "        matplotlib_imshow(images[idx], one_channel=False)\n",
    "        ax.set_title(f\"{probs[idx]}\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "running_loss = 0.0\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(1):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "               .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # SAVE LOGS\n",
    "        # ...log the running loss\n",
    "        writer.add_scalar('training loss',\n",
    "                        running_loss,\n",
    "                        epoch * len(train_loader) + i)\n",
    "\n",
    "        # ...log a Matplotlib Figure showing the model's predictions on a\n",
    "        # random mini-batch\n",
    "        writer.add_figure('predictions vs. actuals',\n",
    "                          plot_classes_preds(model, images, labels),\n",
    "                          global_step = epoch * len(train_loader) + i)\n",
    "        running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "\n",
    "1. ConvNet 3 layers | acc total =  | class 1 prec =  | class 1 recall = \n",
    "2. ConvNet 2 layers | acc total =  | class 1 prec =  | class 1 recall = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_true, results_pred = test_model(model=model, data_loader=test_loader)\n",
    "cr = classification_report(y_true=results_true, y_pred=results_pred)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'epoch': epoch + 1,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict()\n",
    "}\n",
    "\n",
    "save_ckp(state=checkpoint, is_best=True, model_name=\"3d_conv_net_3_layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
